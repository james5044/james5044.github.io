```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

# X and Y data
x_train = [1, 2, 3]
y_train = [1, 2, 3]

W = tf.Variable(tf.random_normal([1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')

# Our hypothesis XW+b
hypothesis = x_train * W + b

# cost/loss function
cost = tf.reduce_mean(tf.square(hypothesis - y_train))

# Minimize
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost)

# Launch the graph in a session.
sess = tf.Session()
# Initializes global variables in the graph.
sess.run(tf.global_variables_initializer())

# Fit the line
for step in range(2001):
   sess.run(train)
   if step % 20 == 0:
       print(step, sess.run(cost), sess.run(W), sess.run(b))

```

    WARNING:tensorflow:From C:\Python\Anaconda3\lib\site-packages\tensorflow\python\compat\v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
    Instructions for updating:
    non-resource variables are not supported in the long term
    0 0.13684158 [0.56293863] [0.77669]
    20 0.08625716 [0.6520391] [0.77034837]
    40 0.07799536 [0.6749203] [0.7370165]
    60 0.07083351 [0.6908194] [0.70265293]
    80 0.06433212 [0.70540893] [0.6696568]
    100 0.058427442 [0.7192593] [0.63818794]
    120 0.053064734 [0.73245364] [0.60819566]
    140 0.04819424 [0.7450274] [0.57961255]
    160 0.04377076 [0.7570103] [0.55237293]
    180 0.0397533 [0.7684298] [0.5264134]
    200 0.036104593 [0.7793128] [0.5016739]
    220 0.03279077 [0.7896843] [0.47809708]
    240 0.029781105 [0.79956836] [0.45562828]
    260 0.027047666 [0.8089879] [0.43421543]
    280 0.024565125 [0.8179648] [0.41380888]
    300 0.022310428 [0.8265198] [0.39436132]
    320 0.020262694 [0.8346727] [0.3758278]
    340 0.018402902 [0.8424425] [0.35816526]
    360 0.016713813 [0.8498471] [0.34133282]
    380 0.015179745 [0.8569037] [0.32529142]
    400 0.013786502 [0.86362875] [0.31000394]
    420 0.012521106 [0.8700377] [0.29543486]
    440 0.011371874 [0.8761454] [0.28155056]
    460 0.010328126 [0.8819661] [0.26831877]
    480 0.009380161 [0.8875134] [0.2557087]
    500 0.00851922 [0.89279974] [0.24369131]
    520 0.007737286 [0.8978377] [0.23223872]
    540 0.007027123 [0.90263903] [0.22132437]
    560 0.006382156 [0.9072146] [0.21092297]
    580 0.005796379 [0.9115752] [0.20101039]
    600 0.0052643656 [0.91573083] [0.19156365]
    620 0.0047811735 [0.91969115] [0.18256085]
    640 0.0043423427 [0.92346543] [0.17398117]
    660 0.003943786 [0.9270622] [0.1658047]
    680 0.0035818042 [0.9304901] [0.15801248]
    700 0.0032530555 [0.93375677] [0.15058646]
    720 0.0029544744 [0.9368699] [0.14350945]
    740 0.0026833008 [0.9398369] [0.13676499]
    760 0.002437017 [0.9426643] [0.13033752]
    780 0.002213337 [0.9453588] [0.12421218]
    800 0.002010193 [0.9479267] [0.11837471]
    820 0.001825688 [0.95037395] [0.11281159]
    840 0.0016581202 [0.9527063] [0.10750983]
    860 0.0015059295 [0.9549289] [0.10245723]
    880 0.0013677087 [0.95704705] [0.09764214]
    900 0.0012421786 [0.9590656] [0.09305336]
    920 0.001128167 [0.9609894] [0.08868022]
    940 0.0010246182 [0.96282285] [0.08451255]
    960 0.00093057455 [0.96457] [0.08054075]
    980 0.00084516424 [0.96623504] [0.07675564]
    1000 0.00076759054 [0.96782184] [0.07314843]
    1020 0.00069713686 [0.9693342] [0.06971075]
    1040 0.00063315063 [0.97077537] [0.06643453]
    1060 0.00057503796 [0.97214884] [0.06331234]
    1080 0.0005222596 [0.9734577] [0.06033688]
    1100 0.00047432331 [0.97470516] [0.05750126]
    1120 0.00043078777 [0.9758939] [0.05479889]
    1140 0.0003912473 [0.97702676] [0.05222354]
    1160 0.00035533812 [0.97810644] [0.04976922]
    1180 0.00032272277 [0.97913533] [0.04743025]
    1200 0.0002931008 [0.980116] [0.0452012]
    1220 0.00026619932 [0.9810505] [0.04307686]
    1240 0.00024176657 [0.981941] [0.04105239]
    1260 0.00021957674 [0.9827897] [0.03912308]
    1280 0.00019942218 [0.98359853] [0.03728444]
    1300 0.00018111907 [0.9843693] [0.03553221]
    1320 0.00016449495 [0.9851039] [0.03386236]
    1340 0.0001493972 [0.9858039] [0.03227096]
    1360 0.00013568565 [0.9864711] [0.03075436]
    1380 0.00012323068 [0.9871069] [0.029309]
    1400 0.00011192072 [0.98771286] [0.02793159]
    1420 0.00010164816 [0.9882903] [0.0266189]
    1440 9.231863e-05 [0.9888406] [0.02536792]
    1460 8.384612e-05 [0.989365] [0.02417574]
    1480 7.614941e-05 [0.989865] [0.02303953]
    1500 6.915911e-05 [0.99034125] [0.02195664]
    1520 6.281067e-05 [0.99079525] [0.02092471]
    1540 5.704633e-05 [0.9912278] [0.01994129]
    1560 5.181007e-05 [0.9916401] [0.01900412]
    1580 4.7054953e-05 [0.99203295] [0.01811098]
    1600 4.2735544e-05 [0.9924074] [0.01725981]
    1620 3.8813203e-05 [0.99276423] [0.01644865]
    1640 3.5250592e-05 [0.9931043] [0.01567562]
    1660 3.2015017e-05 [0.99342835] [0.01493894]
    1680 2.9077159e-05 [0.99373716] [0.01423688]
    1700 2.6407986e-05 [0.9940315] [0.01356779]
    1720 2.3984207e-05 [0.99431205] [0.01293015]
    1740 2.1782877e-05 [0.9945793] [0.01232247]
    1760 1.9783527e-05 [0.994834] [0.01174338]
    1780 1.7968034e-05 [0.99507684] [0.01119152]
    1800 1.6318843e-05 [0.9953083] [0.01066553]
    1820 1.48206045e-05 [0.99552876] [0.01016426]
    1840 1.3460697e-05 [0.99573886] [0.00968657]
    1860 1.2225212e-05 [0.99593914] [0.00923134]
    1880 1.1103076e-05 [0.99612993] [0.0087975]
    1900 1.00839725e-05 [0.99631184] [0.00838405]
    1920 9.158585e-06 [0.9964851] [0.00799003]
    1940 8.317761e-06 [0.9966503] [0.00761456]
    1960 7.5544885e-06 [0.99680775] [0.00725672]
    1980 6.8610148e-06 [0.9969578] [0.00691567]
    2000 6.2314843e-06 [0.9971007] [0.00659066]
    


```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

#Placeholder
X = tf.placeholder(tf.float32)
Y = tf.placeholder(tf.float32)

for step in range(2001):
   cost_val, W_val, b_val, _ = \
       sess.run([cost, W, b, train],
                feed_dict={X: [1, 2, 3], Y: [1, 2, 3]})
   if step % 20 == 0:
       print(step, cost_val, W_val, b_val)

```

    0 6.2314843e-06 [0.9971007] [0.00659066]
    20 5.6592835e-06 [0.997237] [0.00628094]
    40 5.140007e-06 [0.99736685] [0.00598575]
    60 4.6681034e-06 [0.9974905] [0.00570447]
    80 4.239886e-06 [0.99760854] [0.00543639]
    100 3.8506946e-06 [0.9977209] [0.00518091]
    120 3.4972281e-06 [0.997828] [0.00493744]
    140 3.176154e-06 [0.9979301] [0.00470539]
    160 2.8847496e-06 [0.9980273] [0.00448429]
    180 2.6200669e-06 [0.99812] [0.00427357]
    200 2.3796385e-06 [0.9982084] [0.00407274]
    220 2.161056e-06 [0.99829257] [0.00388134]
    240 1.9627212e-06 [0.99837285] [0.00369892]
    260 1.7826973e-06 [0.99844927] [0.00352511]
    280 1.6190365e-06 [0.9985221] [0.00335948]
    300 1.4704256e-06 [0.99859154] [0.00320163]
    320 1.335501e-06 [0.99865776] [0.00305118]
    340 1.2130181e-06 [0.9987208] [0.00290781]
    360 1.1017058e-06 [0.9987809] [0.00277117]
    380 1.000578e-06 [0.9988382] [0.00264097]
    400 9.087644e-07 [0.9988928] [0.00251689]
    420 8.2528345e-07 [0.9989449] [0.00239861]
    440 7.495924e-07 [0.9989944] [0.00228592]
    460 6.808168e-07 [0.9990416] [0.00217851]
    480 6.183822e-07 [0.9990866] [0.00207617]
    500 5.616445e-07 [0.99912953] [0.00197864]
    520 5.101315e-07 [0.9991705] [0.00188569]
    540 4.6331365e-07 [0.9992094] [0.00179711]
    560 4.2079185e-07 [0.9992466] [0.00171267]
    580 3.8220483e-07 [0.9992819] [0.00163222]
    600 3.4715723e-07 [0.9993157] [0.00155557]
    620 3.1525335e-07 [0.9993478] [0.00148249]
    640 2.8639573e-07 [0.9993784] [0.00141286]
    660 2.6009013e-07 [0.9994076] [0.0013465]
    680 2.3624607e-07 [0.9994354] [0.00128326]
    700 2.1456167e-07 [0.99946195] [0.001223]
    720 1.9488219e-07 [0.9994872] [0.00116557]
    740 1.7703665e-07 [0.99951124] [0.00111086]
    760 1.6079053e-07 [0.9995342] [0.00105871]
    780 1.4607927e-07 [0.999556] [0.00100901]
    800 1.3265871e-07 [0.99957687] [0.00096163]
    820 1.2048277e-07 [0.9995967] [0.00091648]
    840 1.09444976e-07 [0.9996157] [0.00087349]
    860 9.9458454e-08 [0.99963367] [0.00083252]
    880 9.034227e-08 [0.99965084] [0.00079343]
    900 8.201831e-08 [0.9996673] [0.00075619]
    920 7.453892e-08 [0.99968284] [0.00072075]
    940 6.769944e-08 [0.9996977] [0.00068691]
    960 6.14942e-08 [0.999712] [0.00065472]
    980 5.5868895e-08 [0.99972534] [0.000624]
    1000 5.074928e-08 [0.9997384] [0.00059474]
    1020 4.6118487e-08 [0.9997505] [0.00056685]
    1040 4.1882824e-08 [0.9997624] [0.00054027]
    1060 3.8038642e-08 [0.9997733] [0.00051492]
    1080 3.4555864e-08 [0.99978405] [0.00049083]
    1100 3.1382005e-08 [0.9997941] [0.00046776]
    1120 2.8521356e-08 [0.99980366] [0.00044591]
    1140 2.5908847e-08 [0.9998131] [0.00042499]
    1160 2.354362e-08 [0.99982166] [0.00040506]
    1180 2.139659e-08 [0.99983] [0.00038615]
    1200 1.942138e-08 [0.9998382] [0.00036801]
    1220 1.7649697e-08 [0.99984556] [0.00035075]
    1240 1.6045025e-08 [0.9998527] [0.00033441]
    1260 1.4568429e-08 [0.99985987] [0.00031875]
    1280 1.3234943e-08 [0.9998663] [0.00030376]
    1300 1.2034328e-08 [0.99987245] [0.00028957]
    1320 1.0939995e-08 [0.9998784] [0.0002761]
    1340 9.9302815e-09 [0.99988437] [0.00026315]
    1360 9.0169765e-09 [0.9998897] [0.00025074]
    1380 8.197376e-09 [0.9998947] [0.00023902]
    1400 7.453262e-09 [0.9998995] [0.00022794]
    1420 6.7751205e-09 [0.9999043] [0.00021737]
    1440 6.150579e-09 [0.999909] [0.00020714]
    1460 5.591238e-09 [0.99991316] [0.00019737]
    1480 5.0740225e-09 [0.99991715] [0.00018812]
    1500 4.6141158e-09 [0.99992096] [0.00017938]
    1520 4.2074e-09 [0.99992454] [0.00017109]
    1540 3.8174335e-09 [0.9999281] [0.00016318]
    1560 3.4717835e-09 [0.9999317] [0.00015552]
    1580 3.1462772e-09 [0.9999349] [0.00014812]
    1600 2.8585394e-09 [0.99993795] [0.00014113]
    1620 2.6001794e-09 [0.99994075] [0.00013451]
    1640 2.3635824e-09 [0.99994344] [0.00012826]
    1660 2.1529043e-09 [0.999946] [0.00012234]
    1680 1.9592685e-09 [0.99994844] [0.00011674]
    1700 1.7839832e-09 [0.9999508] [0.00011141]
    1720 1.6216473e-09 [0.9999532] [0.00010626]
    1740 1.4702772e-09 [0.9999556] [0.00010124]
    1760 1.3321388e-09 [0.99995774] [9.636596e-05]
    1780 1.2073184e-09 [0.9999597] [9.1765e-05]
    1800 1.096945e-09 [0.99996156] [8.740135e-05]
    1820 9.941764e-10 [0.99996334] [8.328461e-05]
    1840 9.0379887e-10 [0.999965] [7.939447e-05]
    1860 8.23234e-10 [0.9999666] [7.572827e-05]
    1880 7.487693e-10 [0.9999681] [7.224938e-05]
    1900 6.8216455e-10 [0.99996954] [6.895308e-05]
    1920 6.2405436e-10 [0.99997085] [6.5832726e-05]
    1940 5.6919525e-10 [0.9999721] [6.289221e-05]
    1960 5.1880117e-10 [0.99997336] [6.0094295e-05]
    1980 4.736733e-10 [0.99997455] [5.7427882e-05]
    2000 4.3180037e-10 [0.99997574] [5.4856606e-05]
    


```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

W = tf.Variable(tf.random_normal([1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')

X = tf.placeholder(tf.float32, shape=[None])
Y = tf.placeholder(tf.float32, shape=[None])

# Our hypothesis XW+b
hypothesis = X * W + b
# cost/loss function
cost = tf.reduce_mean(tf.square(hypothesis - Y))
# Minimize
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost)

# Launch the graph in a session.
sess = tf.Session()
# Initializes global variables in the graph.
sess.run(tf.global_variables_initializer())

# Fit the line
for step in range(2001):
   cost_val, W_val, b_val, _ = sess.run([cost, W, b, train],
                feed_dict={X: [1, 2, 3, 4, 5], Y: [2.1, 3.1, 4.1, 5.1, 6.1]})
   if step % 20 == 0:
       print(step, cost_val, W_val, b_val)

```

    0 1.0729631 [0.93424] [2.328936]
    20 0.21327844 [0.70133644] [2.1827488]
    40 0.18624285 [0.71982473] [2.1115417]
    60 0.16264729 [0.7381688] [2.0452938]
    80 0.1420409 [0.75531644] [1.9833852]
    100 0.12404542 [0.7713409] [1.9255316]
    120 0.10832979 [0.786316] [1.8714671]
    140 0.09460528 [0.80031025] [1.8209431]
    160 0.08261953 [0.8133881] [1.773728]
    180 0.07215221 [0.82560945] [1.7296051]
    200 0.06301102 [0.8370305] [1.6883717]
    220 0.05502812 [0.8477034] [1.6498389]
    240 0.048056476 [0.8576774] [1.6138296]
    260 0.041967995 [0.8669983] [1.5801783]
    280 0.036650922 [0.8757087] [1.5487307]
    300 0.03200751 [0.88384867] [1.519343]
    320 0.027952421 [0.89145553] [1.4918799]
    340 0.024411041 [0.8985642] [1.4662153]
    360 0.021318357 [0.9052073] [1.4422317]
    380 0.018617492 [0.91141534] [1.4198188]
    400 0.016258802 [0.91721684] [1.3988736]
    420 0.014198947 [0.9226383] [1.3793002]
    440 0.012400014 [0.9277048] [1.3610084]
    460 0.010829015 [0.9324395] [1.3439146]
    480 0.009457054 [0.93686414] [1.3279403]
    500 0.008258923 [0.940999] [1.3130125]
    520 0.0072125816 [0.94486296] [1.2990621]
    540 0.0062987907 [0.948474] [1.2860252]
    560 0.0055007683 [0.9518485] [1.2738421]
    580 0.0048038745 [0.95500195] [1.2624571]
    600 0.0041952645 [0.9579489] [1.2518177]
    620 0.0036637548 [0.9607029] [1.2418752]
    640 0.00319958 [0.9632764] [1.2325835]
    660 0.0027942169 [0.9656815] [1.2239004]
    680 0.0024402044 [0.96792907] [1.2157861]
    700 0.0021310395 [0.97002953] [1.208203]
    720 0.0018610402 [0.9719924] [1.2011163]
    740 0.001625262 [0.97382665] [1.1944941]
    760 0.0014193582 [0.97554076] [1.1883056]
    780 0.0012395354 [0.97714263] [1.1825224]
    800 0.0010824942 [0.97863954] [1.1771178]
    820 0.0009453461 [0.98003846] [1.1720674]
    840 0.0008255821 [0.9813458] [1.1673476]
    860 0.0007209858 [0.9825674] [1.162937]
    880 0.00062963855 [0.9837091] [1.1588153]
    900 0.00054987613 [0.98477596] [1.1549635]
    920 0.00048019987 [0.9857731] [1.1513635]
    940 0.00041936044 [0.9867048] [1.1479995]
    960 0.00036623166 [0.98757553] [1.1448561]
    980 0.00031983768 [0.98838925] [1.1419184]
    1000 0.0002793087 [0.98914963] [1.139173]
    1020 0.00024392646 [0.9898602] [1.1366076]
    1040 0.00021302211 [0.9905243] [1.1342101]
    1060 0.0001860344 [0.9911449] [1.1319697]
    1080 0.00016246332 [0.9917249] [1.1298757]
    1100 0.00014187806 [0.99226683] [1.1279191]
    1120 0.00012390628 [0.9927733] [1.1260906]
    1140 0.00010820532 [0.9932466] [1.1243819]
    1160 9.4497635e-05 [0.9936888] [1.1227851]
    1180 8.252675e-05 [0.9941022] [1.1212931]
    1200 7.206881e-05 [0.9944885] [1.1198983]
    1220 6.293876e-05 [0.9948494] [1.1185951]
    1240 5.4964312e-05 [0.9951868] [1.1173772]
    1260 4.7998426e-05 [0.99550205] [1.1162391]
    1280 4.1918676e-05 [0.9957966] [1.1151756]
    1300 3.660743e-05 [0.9960719] [1.1141818]
    1320 3.19704e-05 [0.9963291] [1.1132531]
    1340 2.7919648e-05 [0.9965695] [1.1123852]
    1360 2.438304e-05 [0.99679416] [1.111574]
    1380 2.1294478e-05 [0.9970041] [1.1108161]
    1400 1.859352e-05 [0.9972005] [1.110107]
    1420 1.6237924e-05 [0.9973839] [1.1094449]
    1440 1.418029e-05 [0.9975552] [1.1088264]
    1460 1.2384214e-05 [0.9977153] [1.1082484]
    1480 1.0815743e-05 [0.99786496] [1.1077082]
    1500 9.445054e-06 [0.99800473] [1.1072034]
    1520 8.248161e-06 [0.99813545] [1.1067315]
    1540 7.203065e-06 [0.9982576] [1.1062907]
    1560 6.2906197e-06 [0.99837166] [1.1058788]
    1580 5.493651e-06 [0.9984783] [1.1054939]
    1600 4.797267e-06 [0.99857795] [1.1051339]
    1620 4.189743e-06 [0.9986711] [1.1047976]
    1640 3.6586582e-06 [0.99875814] [1.1044834]
    1660 3.1951872e-06 [0.99883944] [1.1041899]
    1680 2.7904514e-06 [0.99891543] [1.1039156]
    1700 2.4372953e-06 [0.9989865] [1.1036592]
    1720 2.1287356e-06 [0.9990528] [1.1034195]
    1740 1.8588165e-06 [0.9991148] [1.1031957]
    1760 1.6232827e-06 [0.99917275] [1.1029863]
    1780 1.4178153e-06 [0.9992269] [1.1027908]
    1800 1.238294e-06 [0.9992776] [1.1026081]
    1820 1.0811804e-06 [0.99932486] [1.1024373]
    1840 9.4428816e-07 [0.9993691] [1.1022776]
    1860 8.2445575e-07 [0.9994104] [1.1021284]
    1880 7.202621e-07 [0.999449] [1.1019889]
    1900 6.290332e-07 [0.99948514] [1.1018587]
    1920 5.493195e-07 [0.9995188] [1.101737]
    1940 4.7982076e-07 [0.99955034] [1.1016234]
    1960 4.1898505e-07 [0.9995797] [1.1015172]
    1980 3.659739e-07 [0.9996072] [1.1014179]
    2000 3.1953567e-07 [0.9996329] [1.101325]
    


```python
print(sess.run(hypothesis, feed_dict={X: [5]}))
print(sess.run(hypothesis, feed_dict={X: [2.5]}))
print(sess.run(hypothesis, 
                   feed_dict={X: [1.5, 3.5]}))

```

    [6.099491]
    [3.600406]
    [2.6007717 4.60004  ]
    
