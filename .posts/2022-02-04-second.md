```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import matplotlib.pyplot as plt

X = [1, 2, 3]
Y = [1, 2, 3]

W = tf.placeholder(tf.float32)
# Our hypothesis for linear model X * W
hypothesis = X * W

# cost/loss function
cost = tf.reduce_mean(tf.square(hypothesis - Y))
# Launch the graph in a session.
sess = tf.Session()
# Initializes global variables in the graph.
sess.run(tf.global_variables_initializer())
# Variables for plotting cost function
W_val = []
cost_val = []

for i in range(-30, 50):
   feed_W = i * 0.1
   curr_cost, curr_W = sess.run([cost, W], feed_dict={W: feed_W})
   W_val.append(curr_W)
   cost_val.append(curr_cost)

# Show the cost function
plt.plot(W_val, cost_val)
plt.show()

```

    WARNING:tensorflow:From C:\Python\Anaconda3\lib\site-packages\tensorflow\python\compat\v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
    Instructions for updating:
    non-resource variables are not supported in the long term
    


    
![png](output_0_1.png)
    



```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import matplotlib.pyplot as plt

x_data = [1, 2, 3]
y_data = [1, 2, 3]

W = tf.Variable(tf.random_normal([1]), name='weight')
X = tf.placeholder(tf.float32)
Y = tf.placeholder(tf.float32)

# Our hypothesis for linear model X * W
hypothesis = X * W

# cost/loss function
cost = tf.reduce_sum(tf.square(hypothesis - Y))

# Minimize: Gradient Descent using derivative: W -= learning_rate * derivative
learning_rate = 0.1
gradient = tf.reduce_mean((W * X - Y) * X)
descent = W - learning_rate * gradient
update = W.assign(descent)

# Launch the graph in a session.
sess = tf.Session()
# Initializes global variables in the graph.
sess.run(tf.global_variables_initializer())
for step in range(21):
   sess.run(update, feed_dict={X: x_data, Y: y_data})
   print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))

```

    0 0.5701367 [0.798198]
    1 0.16217217 [0.89237225]
    2 0.046129033 [0.9425985]
    3 0.013121156 [0.96938586]
    4 0.0037322433 [0.98367244]
    5 0.0010616274 [0.99129194]
    6 0.0003019667 [0.9953557]
    7 8.589259e-05 [0.99752307]
    8 2.4430758e-05 [0.998679]
    9 6.9490143e-06 [0.9992955]
    10 1.9766085e-06 [0.99962425]
    11 5.6233404e-07 [0.9997996]
    12 1.5993835e-07 [0.9998931]
    13 4.545734e-08 [0.999943]
    14 1.2915123e-08 [0.9999696]
    15 3.6798156e-09 [0.9999838]
    16 1.0488357e-09 [0.99999136]
    17 2.9654146e-10 [0.9999954]
    18 8.4487084e-11 [0.99999756]
    19 2.3149482e-11 [0.9999987]
    20 7.1622708e-12 [0.9999993]
    


```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import matplotlib.pyplot as plt

# tf Graph Input
X = [1, 2, 3]
Y = [1, 2, 3]

# Set wrong model weights
W = tf.Variable(5.0)
# Linear model
hypothesis = X * W
# cost/loss function
cost = tf.reduce_mean(tf.square(hypothesis - Y))
# Minimize: Gradient Descent Magic
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
train = optimizer.minimize(cost)

# Launch the graph in a session.
sess = tf.Session()
# Initializes global variables in the graph.
sess.run(tf.global_variables_initializer())

for step in range(100):
   print(step, sess.run(W))
   sess.run(train)

```

    0 5.0
    1 1.2666664
    2 1.0177778
    3 1.0011852
    4 1.000079
    5 1.0000052
    6 1.0000004
    7 1.0
    8 1.0
    9 1.0
    10 1.0
    11 1.0
    12 1.0
    13 1.0
    14 1.0
    15 1.0
    16 1.0
    17 1.0
    18 1.0
    19 1.0
    20 1.0
    21 1.0
    22 1.0
    23 1.0
    24 1.0
    25 1.0
    26 1.0
    27 1.0
    28 1.0
    29 1.0
    30 1.0
    31 1.0
    32 1.0
    33 1.0
    34 1.0
    35 1.0
    36 1.0
    37 1.0
    38 1.0
    39 1.0
    40 1.0
    41 1.0
    42 1.0
    43 1.0
    44 1.0
    45 1.0
    46 1.0
    47 1.0
    48 1.0
    49 1.0
    50 1.0
    51 1.0
    52 1.0
    53 1.0
    54 1.0
    55 1.0
    56 1.0
    57 1.0
    58 1.0
    59 1.0
    60 1.0
    61 1.0
    62 1.0
    63 1.0
    64 1.0
    65 1.0
    66 1.0
    67 1.0
    68 1.0
    69 1.0
    70 1.0
    71 1.0
    72 1.0
    73 1.0
    74 1.0
    75 1.0
    76 1.0
    77 1.0
    78 1.0
    79 1.0
    80 1.0
    81 1.0
    82 1.0
    83 1.0
    84 1.0
    85 1.0
    86 1.0
    87 1.0
    88 1.0
    89 1.0
    90 1.0
    91 1.0
    92 1.0
    93 1.0
    94 1.0
    95 1.0
    96 1.0
    97 1.0
    98 1.0
    99 1.0
    


```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import matplotlib.pyplot as plt

# tf Graph Input
X = [1, 2, 3]
Y = [1, 2, 3]

# Set wrong model weights
W = tf.Variable(-3.0)
# Linear model
hypothesis = X * W
# cost/loss function
cost = tf.reduce_mean(tf.square(hypothesis - Y))
# Minimize: Gradient Descent Magic
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
train = optimizer.minimize(cost)

# Launch the graph in a session.
sess = tf.Session()
# Initializes global variables in the graph.
sess.run(tf.global_variables_initializer())

for step in range(20):
   print(step, sess.run(W))
   sess.run(train)

```

    0 -3.0
    1 0.73333365
    2 0.98222226
    3 0.9988148
    4 0.99992096
    5 0.9999947
    6 0.99999964
    7 0.99999994
    8 1.0
    9 1.0
    10 1.0
    11 1.0
    12 1.0
    13 1.0
    14 1.0
    15 1.0
    16 1.0
    17 1.0
    18 1.0
    19 1.0
    


```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import matplotlib.pyplot as plt

X = [1, 2, 3]
Y = [1, 2, 3]
# Set wrong model weights
W = tf.Variable(5.)
# Linear model
hypothesis = X * W
# Manual gradient
gradient = tf.reduce_mean((W * X - Y) * X) * 2
# cost/loss function
cost = tf.reduce_mean(tf.square(hypothesis - Y))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)

# Get gradients
gvs = optimizer.compute_gradients(cost, [W])
# Apply gradients
apply_gradients = optimizer.apply_gradients(gvs)

# Launch the graph in a session.
sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(100):
   print(step, sess.run([gradient, W, gvs]))
   sess.run(apply_gradients)

```

    0 [37.333332, 5.0, [(37.333336, 5.0)]]
    1 [33.84889, 4.6266665, [(33.84889, 4.6266665)]]
    2 [30.689657, 4.2881775, [(30.689657, 4.2881775)]]
    3 [27.825289, 3.981281, [(27.825289, 3.981281)]]
    4 [25.228264, 3.7030282, [(25.228264, 3.7030282)]]
    5 [22.873625, 3.4507456, [(22.873627, 3.4507456)]]
    6 [20.738754, 3.2220094, [(20.738754, 3.2220094)]]
    7 [18.803139, 3.014622, [(18.803139, 3.014622)]]
    8 [17.04818, 2.8265905, [(17.04818, 2.8265905)]]
    9 [15.457016, 2.6561089, [(15.457016, 2.6561089)]]
    10 [14.014362, 2.5015388, [(14.014362, 2.5015388)]]
    11 [12.706355, 2.3613951, [(12.706355, 2.3613951)]]
    12 [11.520428, 2.2343316, [(11.520429, 2.2343316)]]
    13 [10.445188, 2.1191273, [(10.4451885, 2.1191273)]]
    14 [9.470304, 2.0146754, [(9.4703045, 2.0146754)]]
    15 [8.586408, 1.9199723, [(8.586408, 1.9199723)]]
    16 [7.78501, 1.8341082, [(7.7850103, 1.8341082)]]
    17 [7.058409, 1.7562581, [(7.0584097, 1.7562581)]]
    18 [6.3996243, 1.6856741, [(6.399625, 1.6856741)]]
    19 [5.8023267, 1.6216779, [(5.802327, 1.6216779)]]
    20 [5.2607765, 1.5636547, [(5.2607765, 1.5636547)]]
    21 [4.769771, 1.5110469, [(4.769771, 1.5110469)]]
    22 [4.3245926, 1.4633492, [(4.3245926, 1.4633492)]]
    23 [3.9209645, 1.4201033, [(3.9209647, 1.4201033)]]
    24 [3.555008, 1.3808937, [(3.555008, 1.3808937)]]
    25 [3.2232068, 1.3453436, [(3.223207, 1.3453436)]]
    26 [2.9223745, 1.3131115, [(2.9223745, 1.3131115)]]
    27 [2.6496189, 1.2838877, [(2.6496186, 1.2838877)]]
    28 [2.4023216, 1.2573916, [(2.4023216, 1.2573916)]]
    29 [2.178105, 1.2333684, [(2.178105, 1.2333684)]]
    30 [1.9748148, 1.2115873, [(1.9748147, 1.2115873)]]
    31 [1.7904993, 1.1918392, [(1.7904994, 1.1918392)]]
    32 [1.623386, 1.1739342, [(1.6233861, 1.1739342)]]
    33 [1.4718704, 1.1577004, [(1.4718704, 1.1577004)]]
    34 [1.3344965, 1.1429818, [(1.3344965, 1.1429818)]]
    35 [1.2099432, 1.1296368, [(1.2099432, 1.1296368)]]
    36 [1.0970153, 1.1175374, [(1.0970154, 1.1175374)]]
    37 [0.99462754, 1.1065673, [(0.9946276, 1.1065673)]]
    38 [0.90179634, 1.096621, [(0.90179634, 1.096621)]]
    39 [0.81762886, 1.0876031, [(0.81762886, 1.0876031)]]
    40 [0.7413165, 1.0794268, [(0.7413165, 1.0794268)]]
    41 [0.67212707, 1.0720136, [(0.6721271, 1.0720136)]]
    42 [0.6093953, 1.0652924, [(0.6093954, 1.0652924)]]
    43 [0.5525182, 1.0591984, [(0.55251825, 1.0591984)]]
    44 [0.50094914, 1.0536731, [(0.50094914, 1.0536731)]]
    45 [0.45419374, 1.0486636, [(0.45419377, 1.0486636)]]
    46 [0.41180158, 1.0441216, [(0.41180158, 1.0441216)]]
    47 [0.37336722, 1.0400037, [(0.37336725, 1.0400037)]]
    48 [0.33851996, 1.03627, [(0.33852, 1.03627)]]
    49 [0.30692515, 1.0328848, [(0.30692515, 1.0328848)]]
    50 [0.27827826, 1.0298156, [(0.2782783, 1.0298156)]]
    51 [0.25230527, 1.0270327, [(0.25230527, 1.0270327)]]
    52 [0.2287569, 1.0245097, [(0.2287569, 1.0245097)]]
    53 [0.20740573, 1.022222, [(0.20740573, 1.022222)]]
    54 [0.18804836, 1.020148, [(0.18804836, 1.020148)]]
    55 [0.17049654, 1.0182675, [(0.17049655, 1.0182675)]]
    56 [0.15458433, 1.0165626, [(0.15458433, 1.0165626)]]
    57 [0.14015675, 1.0150168, [(0.14015675, 1.0150168)]]
    58 [0.12707591, 1.0136153, [(0.12707591, 1.0136153)]]
    59 [0.11521538, 1.0123445, [(0.11521538, 1.0123445)]]
    60 [0.10446167, 1.0111923, [(0.10446167, 1.0111923)]]
    61 [0.09471202, 1.0101477, [(0.09471202, 1.0101477)]]
    62 [0.08587202, 1.0092006, [(0.08587202, 1.0092006)]]
    63 [0.07785805, 1.0083419, [(0.07785805, 1.0083419)]]
    64 [0.07059129, 1.0075634, [(0.07059129, 1.0075634)]]
    65 [0.06400236, 1.0068574, [(0.06400236, 1.0068574)]]
    66 [0.05802846, 1.0062174, [(0.05802846, 1.0062174)]]
    67 [0.052612226, 1.005637, [(0.052612226, 1.005637)]]
    68 [0.047702473, 1.005111, [(0.047702473, 1.005111)]]
    69 [0.043249767, 1.0046339, [(0.043249767, 1.0046339)]]
    70 [0.03921318, 1.0042014, [(0.03921318, 1.0042014)]]
    71 [0.035553534, 1.0038093, [(0.035553537, 1.0038093)]]
    72 [0.032236177, 1.0034539, [(0.03223618, 1.0034539)]]
    73 [0.029227654, 1.0031315, [(0.029227655, 1.0031315)]]
    74 [0.02649951, 1.0028392, [(0.02649951, 1.0028392)]]
    75 [0.024025917, 1.0025742, [(0.024025917, 1.0025742)]]
    76 [0.021783749, 1.002334, [(0.02178375, 1.002334)]]
    77 [0.01975123, 1.0021162, [(0.019751232, 1.0021162)]]
    78 [0.017907381, 1.0019187, [(0.017907381, 1.0019187)]]
    79 [0.016236702, 1.0017396, [(0.016236704, 1.0017396)]]
    80 [0.014720838, 1.0015773, [(0.014720838, 1.0015773)]]
    81 [0.01334699, 1.00143, [(0.013346991, 1.00143)]]
    82 [0.012100856, 1.0012965, [(0.012100856, 1.0012965)]]
    83 [0.010971785, 1.0011755, [(0.010971785, 1.0011755)]]
    84 [0.0099481745, 1.0010659, [(0.0099481745, 1.0010659)]]
    85 [0.009018898, 1.0009663, [(0.009018898, 1.0009663)]]
    86 [0.008176883, 1.0008761, [(0.008176884, 1.0008761)]]
    87 [0.007413149, 1.0007943, [(0.007413149, 1.0007943)]]
    88 [0.006721576, 1.0007201, [(0.006721576, 1.0007201)]]
    89 [0.0060940585, 1.0006529, [(0.0060940585, 1.0006529)]]
    90 [0.005525271, 1.000592, [(0.0055252714, 1.000592)]]
    91 [0.0050098896, 1.0005368, [(0.0050098896, 1.0005368)]]
    92 [0.004542589, 1.0004867, [(0.004542589, 1.0004867)]]
    93 [0.0041189194, 1.0004413, [(0.0041189194, 1.0004413)]]
    94 [0.0037339528, 1.0004001, [(0.003733953, 1.0004001)]]
    95 [0.0033854644, 1.0003628, [(0.0033854644, 1.0003628)]]
    96 [0.0030694802, 1.0003289, [(0.0030694804, 1.0003289)]]
    97 [0.0027837753, 1.0002983, [(0.0027837753, 1.0002983)]]
    98 [0.0025234222, 1.0002704, [(0.0025234222, 1.0002704)]]
    99 [0.0022875469, 1.0002451, [(0.0022875469, 1.0002451)]]
    
